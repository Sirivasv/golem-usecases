{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ML - memory estimation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*I will be grateful for reviews, suggestions and fixes - [jacek@golem.network](jacek@golem.network)*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is a document that explores some issues with memory requirements of sequential-verification algorithm described [here](https://github.com/imapp-pl/golem_rd/wiki/Verification-of-iterative-tasks) (this link does not work) \n",
    "It is part of the ML PoC task described [here](https://github.com/imapp-pl/golem_rd/issues/91)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Remarks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " 1. Neural networks can be very big - that is true. Currently, the large ones are about ~1-12GB of weights data + additionally training data. That is because 1-12GB RAM GPUs are currently dominating both private and commercial segments. (private deep learning researchers typically use NVIDIA1060/1080/Titan X 6/12GB cards or cloud solutions with (eg Amazon 1/2/4/8 GB RAM GPUs) - and a network typically has to fit on one card to leverage the computational power (but mulit-gpus solutions are also in use).\n",
    " 2. It doesn't mean that all networks are that big - although it is currently thought that it's best to max the size and then de-overfit it by regularization techniques, the training times and difficulty of training large networks, plus marginal benefits of using them if the user has only a limited amount of data, mean, that the medium-sized netwoks also have some use.\n",
    " 3. Even if that will not be true at some point, there are others sequential algorithms, to which, I believe, this algorithm can be used, if nothing better is developed before."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the second notebook in this directory, there is a hypotesis that, using some additional assumptions about rationality of agents, only a very limited - maybe even very small constant - number of verification steps has to be done - about 2-5 checks should suffice.  \n",
    "**Important!** The analysis done in the second notebook assumes we can choose any fragment of the solution to validate. However, if we were to precommit to choosing $k$ steps out of all epochs, then after the last check, provider would have no incentive to continue working on it.  \n",
    "It is important then to randomize the number of checks too - in some clever way (ie probably most of the first checks will be done at the middleof computations, and assuming we can only afford to run 2-3 checks, and our analysis in second notebook showed that it will suffice, we have to somehow choose point next)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So, it's just a matter of how much data can we afford to send.  \n",
    " - If it is measured in tens of mbytes, then this would seriously prevent such a scheme to be used in Golem.\n",
    " - If it is measured in hundreds mbytes/gigabytes, then it would be possible to validate small neural networks\n",
    " - If it is measured in tens of gigabytes, we can afford to validate all currently used networks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I've checked snapshot files from PyTorch, Keras and TensorFlow experimentally and there is almost no overhead in terms of storage - ie, matrices take about $\\textrm{size of matrix} * \\textrm{size of float32}$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sending files can be improved a little by using some compression method.  \n",
    "Result of experiment on a small and medium network below. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Small network (4.2M):\n",
    " - gzip - 3.9M\n",
    " - bzip2 - 4.0M\n",
    " - xz - 3.9M\n",
    " - 7zip - 3.9M\n",
    " \n",
    "Medium network (168mb):\n",
    " - gzip - 155M\n",
    " - bzip2 - 158M\n",
    " - xz - 153M\n",
    " - 7zip - 153M"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It shows that there is not much to optimize in terms of one network file compression."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Code:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "#!/bin/bash\n",
    "tar -czvf small.gzip small\n",
    "tar -cjvf small.bzip small\n",
    "tar -cJvf small.xz small\n",
    "7za a small.7z small\n",
    "\n",
    "tar -czvf medium.gzip medium\n",
    "tar -cjvf medium.bzip medium\n",
    "tar -cJvf medium.xz medium\n",
    "7za a medium.7z medium\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At the same time, keep in mind that in our case one state of the network is actually 2 states of the physical network (ie the beginning and the end of epoch).  \n",
    "Although compressors are doing a slightly better job here (probably because not all paramters are changed during one iteration) it is still about ~2x increase in memory usage."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Small network (8.5M):\n",
    " - gzip - 8.5M\n",
    " - bzip2 - 8.5M\n",
    " - xz - 4.9M\n",
    " - 7zip - 4.9M\n",
    " \n",
    "Medium network (336mb):\n",
    " - gzip - 309M\n",
    " - bzip2 - 315M\n",
    " - xz - 306M\n",
    " - 7zip - 305M"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I haven't done any experiments to check if multiple checkpoints would be better compressed together, but I think we can safely assume it is not the case, looking at just above results."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**BUT - there is a possible modification of the original idea - we are using random, small part of the network instead of using the hash to precommit. Then, we don't have to send the whole \"after\" state - we can just compare to this \"hash\" - fingerprint of the network.  \n",
    "It still can be used to precommitment and for verification.**"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
